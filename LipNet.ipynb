{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3573a47-3689-4668-b62f-5c8451b2b4e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24af50c-20b8-409d-ad78-30a933fdd669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List,Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b53fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19e88e-c7b9-45c1-ae1e-f2109329c71b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Build Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f7ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path: str) -> torch.Tensor:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    \n",
    "    # Read all frames from the video\n",
    "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert to grayscale\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame = frame[190:236, 80:220]  # Cropping the frame\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Convert frames to a Torch tensor and normalize\n",
    "    frames = torch.tensor(frames, dtype=torch.float32)\n",
    "    mean = frames.mean()\n",
    "    std = frames.std()\n",
    "    frames = (frames - mean) / std  # Standardize frames\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec735e0b-ec98-4eb0-8f49-c35527d6670a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", '?', '!', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' '] (size = 39)\n",
      "Character to number: [13, 8, 2, 10]\n",
      "Number to character: ['o', 'j', 'd', 'l']\n"
     ]
    }
   ],
   "source": [
    "# Define the vocabulary\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create mappings: character to index and index to character\n",
    "char_to_num = {char: idx for idx, char in enumerate(vocab)}\n",
    "num_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "print(f\"The vocabulary is: {vocab} (size = {vocab_size})\")\n",
    "\n",
    "# Example usage: converting characters to numbers\n",
    "example_chars = ['n', 'i', 'c', 'k']\n",
    "example_nums = [char_to_num[char] for char in example_chars]\n",
    "print(\"Character to number:\", example_nums)\n",
    "\n",
    "# Example usage: converting numbers back to characters\n",
    "example_indices = [14, 9, 3, 11]\n",
    "example_chars_back = [num_to_char[idx] for idx in example_indices]\n",
    "print(\"Number to character:\", example_chars_back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be04e972-d7a5-4a72-82d8-a6bdde1f3ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_alignments(path: str) -> List[int]:\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2] != 'sil':\n",
    "            tokens.extend([' ', line[2]])  # Extend the list with space and the word\n",
    "    \n",
    "    # Convert tokens to numbers using our char_to_num mapping\n",
    "    alignment_numbers = [char_to_num[char] for char in ''.join(tokens).strip()]\n",
    "    return alignment_numbers\n",
    "\n",
    "def load_data(path: str):\n",
    "    # Decode the path and get the file name\n",
    "    file_name = path.split('\\\\')[-1].split('.')[0]\n",
    "    video_path = os.path.join('data', 's1', f'{file_name}.mpg')\n",
    "    alignment_path = os.path.join('data', 'alignments', 's1', f'{file_name}.align')\n",
    "    \n",
    "    # Load video frames and alignments\n",
    "    frames = load_video(video_path)\n",
    "    alignments = load_alignments(alignment_path)\n",
    "    \n",
    "    return frames, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559f7420-6802-45fa-9ca0-b1ff209b461c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_19144\\3419181741.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  frames = torch.tensor(frames, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAADSCAYAAADqtKKSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6o0lEQVR4nO29e2xf9X3//7Lj2Lk4trFDbNzYJBtogYZbkya4VFsH3lKGKIxoa1E2UoZW0TmMEGmlWQdTuzJHmzQolQlbxYKmNUsXqdCBVlBmIAzJubmk4zJCEGljCHa41HESEifY5/vHfvn8Xudpf56f8/744/Px5fmQLH0+fp/zvrze73P89vt1K4miKDIhhBBCiJQoLXYHhBBCCDG90OZDCCGEEKmizYcQQgghUkWbDyGEEEKkijYfQgghhEgVbT6EEEIIkSrafAghhBAiVbT5EEIIIUSqaPMhhBBCiFTR5kMIIYQQqTJum4+Ojg5btGiRzZo1y1auXGl79uwZr6aEEEIIMYkoGY/cLj/60Y/stttus0cffdRWrlxpDz30kG3fvt0OHDhgCxYsoPcODw/bkSNHbN68eVZSUlLorgkhhBBiHIiiyI4fP26NjY1WWprjbCMaB1asWBG1tbVlvg8NDUWNjY1Re3t7znt7enoiM9OPfvSjH/3oRz+T8Kenpyfn3/oyKzBnzpyx7u5u27hxY+Z3paWl1traal1dXSOuHxwctMHBwcz36P87iOno6LDZs2ePuH54eDivfuV7X67dmy/HNlibQ0NDmc8zZ87Meh/WUVb2/0/ZJ598krXOioqKWJmXsdn/zVO2Mn/ihHNQWVmZ+XzVVVfFyi688MLM51OnTsXKZsyYYdnAMv8dx+jl4cdrZnb27NmsbUTugA/v89+9XLA9vHbu3LlZ28O++PZRNmwu2Hg//vjjzOfTp0/HyrAevNfj5Y3rhs2bx69Ls/i8oQxRNr5vWParX/0q8xlPQn0bAwMDWfuGY/fyRrnNmTMn9t2PH8fo+8PWHrb/4YcfZj7j+p41a1bm87x582Jl/l1TXV0dK6urq4t992vT12kWn+Py8vJYmX++cd78emPPEK5vlLEfM8qUrTc/NzhPHnyf4ncPvt99+1jm5YH99GMMWQv+WnxmX3vttcxnfC/hs+DrwTL2NyMpUUIFyeDgoH3/+98fsXZHo+Cbjw8++MCGhoasvr4+9vv6+np74403Rlzf3t5u3/72t0f8fvbs2aMusHw3Eezly8j18mWbD9amL8MXgC/DOv2DxF7i+MJhDxkuVrb58HOCC6yqqmrUfmJ7SBqbD3Zf2psPHK9vE1/GrN9+ntj8jnZvtmvz3XzgfPvxh2w+UP7+DxmO0deD93lwDXm54UuVjR/HyOTvQdn75x3v82XYF38tPt/sOcVr/feQzYcfL3uG8H3C5i3kPeHHxJ49tvnI1bd8Nx/+uWVrkb2zsE4/T9gXNg4sY38zkpJ085GtD6NRdG+XjRs32rFjxzI/PT09xe6SEEIIIcaRgp98zJ8/32bMmGF9fX2x3/f19VlDQ8OI6ysqKkbs8CcT+Z7E+F0t7kaT1om7Yb+rxqNOdvTIwP+M/NEgHnXjcauHnRDhrtqPgx3n4xgZvv18T8HwXvafEZsbhB2FMpWE/w8rRKbspCnEyDvpf1QoC5xTpoL0p2soQ/afKB5hZ+sPnq7mqzploEzZ6YL/jx7H5FUteOpYU1MT+85OSXy92L5/pnAufN+YCjCXTJOuRQZ7n2EdISeEHtZvrMfLjZ0S4LPu2z9+/HiszD9TeB9TASJsvfsxsWdmPCj4yUd5ebktW7bMOjs7M78bHh62zs5Oa2lpKXRzQgghhJhkFPzkw8xsw4YNtnbtWlu+fLmtWLHCHnroITt58qTdfvvt49GcEEIIISYR47L5+PKXv2zvv/++3X///dbb22tXXnmlPfPMMyOMUBnDw8OZY6+xHJOPlTTaZganzLArBHacHHL06Y8C+/v7s94XYgzKjkJD6klq4MsMIHMdu/u+YV/8PGJf/LEpliW1Rk9qNIp9MeMqIf+dqZIQNhdMJcPuCzGIY/LwR994JM3UTMzgmKkamIE3MxxlnglePYP15DIo9tficbqXMfbNrz806vRjRNWC7yu+F3BN+bXBVBSoWvDgeNk7w7c3FgcC/x1VJF7tgm0wY3PfHlO75FKz+HIsY96TTP6hRqahjMvmw8xs3bp1tm7duvGqXgghhBCTlKJ7uwghhBBieqHNhxBCCCFSZdzULmNleHi4qLYe401S3Tbq6HLGy09I0npYP32kRjOzX/ziF5nPTU1NsTKmk2UubehS53XyLOgW4nWizI4ml56TuQl6mG4Xx8TsUdgz4K/N1W/ffkhUyaR6XxaAC8H2/DhYADJ0rfbfUW5og5GtTuwL2tz4NljwKITp4L09DsqNBRnz17KopXgtc6Fk84ZzwdYJW6csQBZ7TliQM2aLxNZXCDgm9iyw91tSOxIWpTdkTNhPFlG2mOjkQwghhBCpos2HEEIIIVJl4pzBJCTfo7+JBnN99IS4oTLZMPdK1rekbrdmcTUMql1C8jn4Y0Nsw9+LR49MJZP06DXXdSz3A5MpiyrJ2mQJ2rJdNxr+uBWPXlm/Wb1sTCHPop/vkEi8XkWBrsXMfdm3x5KemcXXH8vRwtYplvlIpUztEuICz5JD4vx6FQ1zF2frgqmgcqlrfPsob+aGzNQX46GODlHrJo2gjP08efJk5rN33Tbj77qpgE4+hBBCCJEq2nwIIYQQIlW0+RBCCCFEqkxYm4/S0tJEGQ4nk52HJ2m/Q7KjMsZDR4p9QdfbpPexrKcsm2NIKG7WfohMfV9Z9kwkqctqiHufL8tl88FsCVj7vl60q/DgXLB+41pkmZK93hvbTzoOlI2fJ9Szoy2Dn2/mBszsr5irKQv9zlwmc+HrYVnDcUxeHt42Ba9lWbND7K2YbHD8LNQ9ey5DwoQnDUWOtjp+DWN7zP7GjwPXnnenDgmLztZJyBoab3TyIYQQQohU0eZDCCGEEKkyYdUuUz3CadIIfePlYhVyFJoNdmSLGRpramoyn0OOZZm7HcIiZXpYtMBcWV3HQ7XC+sZcq5kbbogbcLY6R6s3G+yIHNs+depU7LtXtfT19cXK5syZk/lcV1cXK2PH0t6FkR2D54K52jKYGgDH70mqvgjJzoow9YkH3XdZlF5WZ8izyFx9mRt2Ujf3XHPPMjz7+WBZZUPUPMeOHctaxiIYTyT1Sb7o5EMIIYQQqaLNhxBCCCFSRZsPIYQQQqTKhLX5yOZqO1XsQJhOOKldQ4i7bIiNR75t+Lnp7++PldXW1mY+53KRZVkYk2ZzDLGVYWGbcW683hn1vr79ELuWpOGYmatnLnz7rN+oS/ZtYPtJQ2Ez2yCz+FpBewHmJspso9jz5efQ25SYjdSt+3qY2zeu07lz547anllcpizUPXMnRXkzmxf2fsEylg3YjwPv8/OG88vWKY7DX4uu1b7f6Jbq55G5wOfK6prU3i7f9AFYp7f/YTYtU8HGA9HJhxBCCCFSRZsPIYQQQqTKhFW7THVXW+bSlZSQqKX5ugnm2x+Mdrpo0aJE9+XqC7s2qUqGkSurqu8P1slkzCInsjqSqo9yZUBNKjeWkZRFowypk7l3ovzZUX++rpcerNOrS8zifWWRcbGe2bNnJ+rnvHnzYt+9+iBkLlBF4dUS6DLL+sNUcCwztb821xpmz1DS9RYy9/lGOGXg3CR9Fpi6iqlDpyI6+RBCCCFEqmjzIYQQQohU0eZDCCGEEKkyoW0+RtO/hbiFFiKE+HiR1AYghBC7DhaemWWPZHV4maKrrddt4n2oP2Xulax931emLw3J6src9lBH67+z7LT52rGwOtGFkLkoswy0zNWYzTfT8+dal15/f/7558fKvA0Es49gdeJ4vRtqVVVVrAxdOP21zEUYbRD8GsY2vNywzN/H3HBz4fuN4/futDhe/5xi+/5avI9lTU6SnfwcfozM1ZbZNOFzmdQ9frTvHpbqwNfLbFzwPp+GYiqGUGcE/wV+8cUX7cYbb7TGxkYrKSmxJ598MlYeRZHdf//9dsEFF9js2bOttbXVDh48WKj+CiGEEGKSE7z5OHnypF1xxRXW0dExavnf/d3f2cMPP2yPPvqo7d692+bOnWurVq2iFu5CCCGEmD4Eq12uv/56u/7660cti6LIHnroIfurv/oru+mmm8zM7F/+5V+svr7ennzySfvKV74ytt5a/q6WY7l2opLvcSaSS9WQFC9T3Gz640Uf7dRspJrFHxPj0WtS2LEsO4bGCI8oN18vujCy9plMWWRU/x3XrB8HluGROVOf+HpYFFNUc/ijZsxi7I/9c/3j4VUPDQ0NsTJ/ZO7dV7FvOH6fKZdlWc0VbZdlzq2vr898xnXKXEG9Oy/LcIsqgsrKyqztoRqEyc2vKVzffh5xTpksfJ255tv3nWXHxTXs3xPYPlPVJs0gjrC+sfcLqgeZ2zOTVUj25Xzdcn0babv2FtTw4dChQ9bb22utra2Z31VXV9vKlSutq6urkE0JIYQQYpJSUIPT3t5eM4v/R3Du+7kyZHBwMLYb9P+xCCGEEGLqUXRX2/b2dquurs78NDU1FbtLQgghhBhHCnrycU5f29fXZxdccEHm9319fXbllVeOes/GjRttw4YNme8DAwPTYgPCdMLMTZG5eyV1nx3t3mywrJOIbx916d4tEW0+QkjqbshcP5mcWEhrs7j7W4iOlLkJeh04s7kIcdFloGz8mFid6GrK+ubtGqqrq2NlLKQ52tx4nTj2jWXD9d/RHuLEiROZz+jOiOHV2ZpmmWtZ2Gwvf7Sr8PXgeL1s0P4G+71gwYKsZczOhdkxMVsZL2NcX2if4svxWvZe9DYgLPsxc9FFcC36Z5qNkb1DcL1lq9+MZ9z17YfYf4RQzBDuBT35WLx4sTU0NFhnZ2fmdwMDA7Z7925raWkZ9Z6KigqrqqqK/QghhBBi6hJ88nHixAl76623Mt8PHTpk+/fvt9raWmtubrb169fbd7/7Xbv44ott8eLFdt9991ljY6PdfPPNhey3EEIIISYpwZuPffv22W//9m9nvp9Tmaxdu9Yef/xx+8Y3vmEnT560r33ta9bf32+f//zn7ZlnnhlxnCqEEEKI6Unw5uMLX/gC1ROVlJTYd77zHfvOd74zpo5NZ5hdBStjulzUpXqdJUtrjbBQ3Cy8ufdiyhVenYVDZvEEWIhlZp/A7DEQJn8Wr4OlCvdlOE9MB8/iNSBexixVOvab6fm9bh3/ufCxDrCM/SNy7Nix2PejR49mPqMu/aOPPsp8ZrE88D4/RrRjQVh8FG9ngOuNhcZmYfi9TPEZYnJDdbW3h8EyH8Ie7XH8OmKxW3AtsHcPjtG3gXJLGseJ2WKxGDv4fLH1ztpHfD0493687Hlm7eUbeymEQsS+Cqmj6N4uQgghhJheaPMhhBBCiFSZsFltpzr++I0dw7NjfpY9MYSQIz1/LXP/Ytkb8Ygc3Qb9uFimx6TqISTEJRnx4wqRGztCZcfQ+boW4/GuVz0wdRlzNca++CNs5vqJfcOAg35tfPjhh7Ey5k7rVSY4396dlrk3YlBDpgYJUV2yUNxsvvNVz6GKxMsYQ49/6lOfynzGcPbMnZetRV/2/vvvx8pQ7eXDvbM2cIwhKSLyvY69N5K+i7EOlmHZ14mutlMdnXwIIYQQIlW0+RBCCCFEqmjzIYQQQohUmV5KpgkEc1n1MLdUFm4by5jrZ6604tkIsTHxOmnvImlmVlNTE/vOXPqYm6KnUPYwzF6AkW84e+YmyNpG9z7Us3s5svWG4/V2Fkw/z+wx0Abgvffei3337rU4Dm/zgWvB27V4Gw/Ep3owi8sU1x7K348L7Sp8X3Fu/LVou+BtMLA9ZlPkyzCEeIj90TvvvJP57N1uzcwaGxszn32IdrP4/GPIdj8XuNZDwo17exD27KNM8w0Tzp4FnFNmu8Hu83ODsvD9xrWfhnttMdHJhxBCCCFSRZsPIYQQQqTKhFW7DA0NJT6qHivFON5iR+hJo8QljQZollsNk0/7DHZ87aNWmpldeOGFWe/F49RCyCZX1k0Pc33FNlg2YlbmwfH671gWMocsk2hSl2W8z6tWsKyvry/z2R/zm42MYuqPovE4+8iRI1n75u9DVZ7PnLx06dJYmXcvzaVW8ypAdBH2a+G8886Llfl6USXlZYWyYJGA/XeMzMqyw+Kz78f0q1/9KlbmVWT4XHoXXewbU62g27eXMa5T5krvx4Rl+apDGSGqnKT14vpm93nXWxYxd7Kikw8hhBBCpIo2H0IIIYRIFW0+hBBCCJEqE9bmI02YG2ZIuO3Qe8dKvm6ghSLERdjLCUNoowujD/HMstqi7Jl9BCNXluak9zF3Vn8tywiKY/K6XmZjgrA2sMyvf+ZC6N1e8Vq0QfB2PehqyzKJYhvelsPbKiA+i65ZfEwffPBBrIxlo0UXTm+vgWvau+ledNFFsTJ/LY7Jr3fMVOtlgWV+TCgLtKPyMka3WF8vjjdbHWZx+xQM2e7BtcDsmBA/RmabFZKSgtmjsND3SNKQBCyL76lTp7Leh3ORbwiEyYJOPoQQQgiRKtp8CCGEECJVpHYZhfFWl5jlH3GTwaKmjkd7IfijSFSz4LG0d4VkbqDsWBKPPgulkmJugkndYvE+Pw50S2RqJub2i+37o2fmpoluob4/GJnUH/3jfX6OUUWA8+aP89G9s76+PvP53XffjZX5MX/605+OlXnVwi9+8YtYmXeZRRdZPBb348IxetdTBlPNoqvtW2+9lfmMWU69qgNVl2+88Ubsu1fXYRTX+fPnZz5XVVXFynykWGzj8OHDmc+oEvKgCgyv9Wog9p5i6hl0j2cyZhmGQ2BtMFdYP0Z81yXNGj0VVTA6+RBCCCFEqmjzIYQQQohU0eZDCCGEEKkimw/j+rRcthMskyzD6w9ZCG+E6fk9aYSMZ/1E9zaWjRZDY/vMmqhL9feGuEj7/jDdqs+qaTZybXg9NNbjx8hcjZk9CHMfRlc8r/dm7oQIjsnbOaBdg7dJwHnyobnxPg/eh663TU1Nmc/Nzc2xMp9JFcfvbVeuuOKKWJmXP9qKeBsEtIdAF1K0T/L4caHthgcz7vo1jTYmXjZom+PHn8umybeBNje+HrQr8Wsanz2/3tDmwssN1xdLtYD99vONZb5elJt/bvPNcIuwFAkIyz7tbWfQpsuPF8uYGzSSr01boWSVDzr5EEIIIUSqaPMhhBBCiFSR2mUU0nC19cdk+WZuDSGXK2Yh6mRlLIomup/542Y8svX1MnUZqiGYGiokiimDuTr77yFqNj9GpoJCWaAahKlFvGwqKytjZb6v2M/q6urMZ1St+L6hqyUe2aNrZra+LViwIFbmVSaorvFceeWVse/nn39+5vOiRYtiZahm8SobLPNuqnhE7tUCPsMvXovz5lUk6CJ86aWXZj7jeu7s7Ix99216d2WzuNzQ1di7waJKxl/rZWgWn2OUE1NdYhvsvejXFD4Lfn3jOvXrLZd6gj1THvZ8syi5IbB3T6HUJWn8rcuGTj6EEEIIkSpBm4/29nb77Gc/a/PmzbMFCxbYzTffbAcOHIhdc/r0aWtra7O6ujqrrKy01atXj9j5CyGEEGL6ErT52Llzp7W1tdmuXbtsx44ddvbsWfvd3/3d2FHhPffcY0899ZRt377ddu7caUeOHLFbbrml4B0XQgghxOQkyObjmWeeiX1//PHHbcGCBdbd3W2/+Zu/aceOHbPHHnvMtm7datdee62ZmW3ZssUuueQS27Vrl1199dWJ25oxY0ZGr8nC2uaL15kynSTC+sLc30Iy0BbT/SmEkH4ymwd0RfQ6Y++KhvWgvtLrgdFtzeuBWbZQhM1TyBwmdcNmOli29tDGBfXMzEXZ6/kxA6p3g8U2vK1OT09PrIy5qPrw3tg31ga6nnqXbMQ/3z5cv1l8jOhajaBNhGfhwoWZz7imvOst2id4maK8fQZeDH3u3YlxfaN9iLdzwDDwvk208fHywL552xG0v/Hg3KNbLLPdCMmym7TMg2ufudMyOznmyo9jYG7YnhDXWiRf2418M3oXgjHZfJwTam1trZmZdXd329mzZ621tTVzzZIlS6y5udm6urpGrWNwcNAGBgZiP0IIIYSYuuS9+RgeHrb169fbNddcY0uXLjWz/0vYVF5ePiJwT319fSyZk6e9vd2qq6szP/6/AiGEEEJMPfJ2tW1ra7NXX33VXnrppTF1YOPGjbZhw4bM94GBgdQ3ICxq6VhUIEnvxeOt8VC7pO1SFaJmYllu2VEktsGyynrVWkiGSFwb/t5CqQOTzg1G3wxx4fNqCJSp/45qLu8Gi/30qgaUKbrTevA43883jomp0vy6OXr0aKzMu3CiK7c/XUV1CXPJxvb9mNFl1Ksa8DSXrRsvY+ybHy+OCd2V/TsUVUfeRRrnwteD/0T6enCdMNdqHK9XUaDKNSnjlaU76bPIIgpjHX6Mxc4uHkI+f4dC7slr87Fu3Tp7+umn7cUXX4zpPRsaGuzMmTPW398fW7h9fX0j9K7nqKioGPFSFUIIIcTUJWgbFkWRrVu3zp544gl77rnnbPHixbHyZcuW2cyZM2MBbw4cOGCHDx+2lpaWwvRYCCGEEJOaoJOPtrY227p1q/3kJz+xefPmZew4qqurbfbs2VZdXW133HGHbdiwwWpra62qqsruuusua2lpCfJ0EUIIIcTUJWjzsXnzZjMz+8IXvhD7/ZYtW+yrX/2qmZk9+OCDVlpaaqtXr7bBwUFbtWqVPfLII8EdGxoayugK0wg37pksrq4hpO3qi/L1dgW5wsl7/T3qsllW36S2HKiDZm6oTBasPSZf1j7CMpn6vqLLKGuf2Xygvj5be2ZxGwBsz9eTK+Oun2PMwOptG7Aeb4Px9ttvZ20f7SP8vKHNA+LtLHx2UrO4myraini3YOy3r4fZO6E9xMGDBzOfcV3ivHm1N7rTepdZdOf1thwsLD6uIZYNF/FzjDY++dpmMfK1s8g37UUut/ek7U11gjYfSYQza9Ys6+josI6Ojrw7JYQQQoipy+QxvRVCCCHElGDCZrUdHh5OzT10vFxtWf+ZS994EJJJtRDR8kJkiserPkMqRmf0R8FMfcGy4SK+fbwuZJ7YGH0Z6zeWsay2HqZKGe17UrwLKbqTepUFeqz5+UdVCuLVN+iW6o/6MVKmV7ugSsSPF+Xm5wYjgyJe/hhhleWsYuoDr2rB9eWP6PE+vzbQlRlVJF4Nh1ltvechqjV9myFryL9fcJ2wbMz5viNRNv5avC9f13q8z6tTWNZuXO9J20c5oRynGjr5EEIIIUSqaPMhhBBCiFTR5kMIIYQQqTK1lUrjANP1pR3CfLxIOqaQjIj+2lzh5L3OFN3WvCsgc+Fkel9mc4F1sjDhrJ6Q0OdJbUUQphPHMj8OZo+DdhVeD412HX4eUU5+nnK5Fvv2sd/eJgH77dtEuTEbAGa7gGXelgLHyGwXfD1YZ9I5RTdYFiKfhTtHuw4/DgwZz9y+vSxCbKGYrRLOqf/O2sB++muZTRm2l69NF+sPunb7+0Js4cY7q2yx0cmHEEIIIVJFmw8hhBBCpIo2H0IIIYRIlUlh88HsBQoB6vJCYnsktfNgdRYjrG4hYnmE4PWuGJOA9QV10knnH3WrXifLUqOz+AG52vB2HmxNMb06ts/01V53j7YqISHk/Xdm44J4XT7229eDIczRHsbb+GDfvL0CtuFljPYR/lqUjZ9TFkIc+4Nl3u4C14n/jvEa/PpDGxsvKxyvt7nB5wDtcbxscC16eeCYmK0Ms81ha5HZfGBaAF8v2rEwW7SkZQh797BnCsfvx4hxPlj77H0aEhZ+PGwP/Robj79ROvkQQgghRKpo8yGEEEKIVJkUahdPvsc/Ey2LLTt69xSqb4VSVyU9imMuoyxstFn8OBuPMH0ZHosmzYiJ7Xk1AB7D5xtimbnUIeyInNXhj3qTriezkUfGzL0xaVZfFhaeuUWa8ay6fm6qq6tjZV7VgKHP8z2+Z+6O6N7K1De+DVTz+f4wtR5zbcYyhK0p9mywMTEXZT9PudQFzPXUw9Z0vukpcOwhaSdY6HX/LOJ8M/x9qJ5koe6x/XxTW7C5YM9+tvdUiMpHJx9CCCGESBVtPoQQQgiRKtp8CCGEECJVJqzNRxRFY7Z3yNflaDzCpDN9WjHC6I63nUuIu1uI/tZfy3TCIXpudl++btdYj9eJo6upd7fE9pjc2Lphrp9M78vsb5jemckt1/x6N02sh8nU2yQw920WbjuXrYyXMXN1Zq7dIanR/bUoNz/GXCHbffvozuvB0Ou+jZAw5dnaNhs5Dm+vEvKuZXPFXMn9HDL3YbyX2Ybh+wRt05Li5xHbY7ZJuKa87Uih3u3MHqQQrr06+RBCCCFEqmjzIYQQQohUmbBql5KSkjGrI5JGiGPHXblgffT14HXsmNBTKBVQiOvneLTH1CUh2SuZW2hS1QqLIsqiSiIh68QTkqnWrxsmN7wPr/VH78zVFkk6RqYGwAiXIeNgffH1oNrFw6LW5lLPMbUTk42/lqlEcH35+/Bo3X9naj0zvqbZUT97hliEU18ncwMd7d5sMNUhiy4c8n5hKjFct0ymx48fz3wOyUzNXLJzuVPnQ76uxUn/zgVFZU18pRBCCCFEAdDmQwghhBCpos2HEEIIIVJlwtp8pAnTZ43FbSlpGOEQt9SJTNLMkrlsbPx8oC6bhbvGa5Pi9bdMzzvad48fF3OZZS6bzL0R2/b15MoUzDLQMhdGphNPqrsPCrkMsmEhvX29hZj7QsLcl5n9UyFSBGD7GF6eZfxlWY0LlRIixFYpaV+YjU0Ivl42Nyg3L/8Qu6V8w8Qj/jnFdxh7L4935lpG0F+5zZs32+WXX25VVVVWVVVlLS0t9tOf/jRTfvr0aWtra7O6ujqrrKy01atXW19fX8E7LYQQQojJS9DmY+HChbZp0ybr7u62ffv22bXXXms33XSTvfbaa2Zmds8999hTTz1l27dvt507d9qRI0fslltuGZeOCyGEEGJyEqR2ufHGG2PfH3jgAdu8ebPt2rXLFi5caI899pht3brVrr32WjMz27Jli11yySW2a9cuu/rqqwvX6xRBlUzI0ZS/lkWALEaEUw8bU1IXq1ywSJGsXiY3VDV4dzc8MmURL/0RKh5fhxwLs8iZSd0Ucbz+WpYtk6knsBzn1I8f22Cunx7mTprLnTNpltWkbqC58PWwSJHsPrPkqlSm5mJZdJnLJpM3wjK55nKL9SRVbTAXWfzO3KfZGJO6r4bC1GX++4kTJ7LWERK6wV+bK2qtJ+TvUtJ371iiO+dD3rM0NDRk27Zts5MnT1pLS4t1d3fb2bNnrbW1NXPNkiVLrLm52bq6urLWMzg4aAMDA7EfIYQQQkxdgjcfr7zyilVWVlpFRYXdeeed9sQTT9ill15qvb29Vl5ebjU1NbHr6+vrrbe3N2t97e3tVl1dnflpamoKHoQQQgghJg/Bm4/f+I3fsP3799vu3bvt61//uq1du9Zef/31vDuwceNGO3bsWOanp6cn77qEEEIIMfEJdrUtLy+3iy66yMzMli1bZnv37rXvfe979uUvf9nOnDlj/f39sdOPvr4+a2hoyFpfRUXFuISRHS9yZUWcjORrczJeLsJM7+p11HPnzo2VzZ49O/MZs0wyPTtzb2QujEhSt82QcPK+PWZzgS6SIeH0mWxCdPtJy5guH10YvQ0GC9PObCdYRulcbsf5pkFg9gl+jExOTOee673j+8rCyzN7DLRB8Osv5FnHegrhXsrGH7JmWXh9fPY//vjjzGdcp6zOpOsmjRQYY7FhLDRj/msxPDxsg4ODtmzZMps5c6Z1dnZmyg4cOGCHDx+2lpaWsTYjhBBCiClC0MnHxo0b7frrr7fm5mY7fvy4bd261V544QV79tlnrbq62u644w7bsGGD1dbWWlVVld11113W0tIyaT1dhBBCCFF4gjYfR48etdtuu83ee+89q66utssvv9yeffZZ+53f+R0zM3vwwQettLTUVq9ebYODg7Zq1Sp75JFH8utYWRk9Zk5C0mOrELUDi1zJoseNpc3xIOmxacjRH3NnZS6MrA08+vTf6+rqYmWVlZWZz+gK57+H9CXfSK35uq2xrLYsc2uuyInMFZdFZ/RjZGuWtZcraiw7hvfH2yyqZMgxvM/wi+Qb8ZSpXZgqiz2HTCWCaw/nxj8LOPfz5s3LfEbVJXPJZhl+Pdg3phJk2b5xvtn7xf+tCFElsHljLuEhmXPTJsR9POnfqKSqo5C/a0F/3R977DFaPmvWLOvo6LCOjo6QaoUQQggxjZicSUSEEEIIMWnR5kMIIYQQqTLpstqGuCkyHXy+pOGqNB4uVmm4BLNw8iyMMNOJo82H1wNjPV63jLpsH3oddf5+TlHPjG7gfv3hGH1/UF+cVP7MVmQsemU/RqzHj5HZQ+A8MfsAP9+og0bbFRZu24My9bpsXCfMdsC7TOZqw4PjzzcUuB+jdw83i8sKZcpsftizgLZR3uYD6/T14Drx/WZ2HLlsetgc5/s+ZXYkDLSH8OsInwXmXpsvhfobxcLyM9tJZg8y3ujkQwghhBCpos2HEEIIIVJlwqpdPvnkk1GPhAp1TFTMyG5m+UcDTTuiaiGOQRGWOdWMq2j8kS4eg/pr8TjZf8cy5l6HfWVHuiw6oi9jkQzHEp2RwbKeenlgbib/vOHY/RhxnpiKAI+B/TgwMq2vB1Vp/lqvVjOLq1ZwLnydWIZjTJrllkWmRfw42DplkUFxDaFsfD3nn39+1r4wdU3InLLnEvvGVA0swqqXMap9mLxDMoj79pkqMd/Iv0jSCLpjwc9NSDZkLzd2XbZ7cqGTDyGEEEKkijYfQgghhEgVbT6EEEIIkSoT1uajEKRh11HMrLYsk2ch682HEFdbxOtM0QYDdfvZQBdG716IdgVe758rvLaXDeqy83WFZSHbk+qyEbyPZY5O6t7K5g3XHluL+WZDRpsDFnqe2WP4+UabnpCMxyFuuR7fb1xDc+bMyXxm7qz+utGu9XOFzwLD34d1+n6j3U5IxtuQtZENbIPNRUjI/Nra2sxntF15//33M5/Z85TUPgIp1N8StvZC0oOMNzr5EEIIIUSqaPMhhBBCiFTR5kMIIYQQqTKlbT48xbTNGAv5pmbPRbHjnHiYjhj1pwMDA5nPLEYA1un13qhn93p/jPuA9iEe1IknJST9u5+nkNTsITEDkqZKR7zcWOwStMdAuXndelVVVawM58PDwtn7vrEU76jXx7gevpyF18b7WGwJbx+BsvHrFO0KmI0Lu5aF10Y7JfYMsTUVYu/E4kf4dRNiq+HBefJ1YhnKsampKfMZ7YHefffdzGccb9IYIEhIDBIPyobNMbNBYe+QpPZmSeMUITr5EEIIIUSqaPMhhBBCiFSZNmqXYsOOzKc7ubJgelhGUg8eISYNvY6ZLFkoeKYSYP1hR/SIP4ot1JrJN/wzy4aLc8jCRmM9LNw5upR6fJv5upkzt2szriJg7uN+zFjmQ9ijGyy7zx+t4zE7C73P1CWoAstX7cLqRLkxN2gWCjzp8X5ICHFMJ8DG79cYluXrXsvqZPUzl3B8FvLNqM3aK8S7SCcfQgghhEgVbT6EEEIIkSrafAghhBAiVaaUzcdEch81m1j9SbsvITpQ5m7H0mrjff5a1Ht6XS662no7ErQHQRsTbwOC9gIsHTgLGe/tDFBf7ucN7/NtoA6W2UCwuQlJ487Cwoforz3MfZmtYWbjw/TTOF608WFzw/D1oj2GD/WPZczt2a9NZuNhFrddYmHpx+KinRTsq5cxsxVithusjNmReNmbjbT58ODz7d382X0hMHn7MaKdGHuG8rV/yteuQ662QgghhJgUaPMhhBBCiFSZdGqXiaTKKAbsWCtpdtKxkDQCZog7Jx4T+jlmUffQLdK7LTL3XRyDd+fECIjsWBhVMt71NETtwCIwMvVB0oyz2CabGxY1lrk+snpyPbN+HExFEPLsJ11/6LKKa5G5vrL2mNqFuQ/7+7Bv/j6mjkRCIpyyOpk6lIH1eBUCqrlQvZCtfYS573pQ7cLGgWqXnp6erG1UVlZmPrM1FRJF1JNr7Sd1rWbv2qT1FwqdfAghhBAiVca0+di0aZOVlJTY+vXrM787ffq0tbW1WV1dnVVWVtrq1autr69vrP0UQgghxBQh783H3r177R//8R/t8ssvj/3+nnvusaeeesq2b99uO3futCNHjtgtt9wy5o4KIYQQYmqQl83HiRMnbM2aNfaDH/zAvvvd72Z+f+zYMXvsscds69atdu2115qZ2ZYtW+ySSy6xXbt22dVXX524jSiKxmzfkXYIc+xvUpenfO9j4Z7Hi6S2I0yXmqsOP36WPfLDDz+MldXV1WVtH3XLHm9ngDph7Ovx48dH7UuuNvyYsM5sGSLxWubOinWGuNsldW8NcX30fcuVmZm5SDP82gixY/Gg3RDL8spAGfo1hRlnWb+93FhfctlcYJtJYXPqv+Na9H3LZTd14sSJzGdMZ5DvO9u3iX3ztmAYzh7x48C1cfjw4cxnHFNjY2PmM7ryMxsfD8tUy+xvcpHUHiTEnrBo4dXb2trshhtusNbW1tjvu7u77ezZs7HfL1myxJqbm62rq2vUugYHB21gYCD2I4QQQoipS/DJx7Zt2+xnP/uZ7d27d0RZb2+vlZeXjwjAUl9fb729vaPW197ebt/+9rdDuyGEEEKISUrQ5qOnp8fuvvtu27FjxwiXuHzZuHGjbdiwIfN9YGDAmpqaClJ3GhTC9ZdFrkTSUK0UAqZaYUf0ZvGjTxZx9P3334+VLV68OGsb2erH77ncIn29eGTs58YfLZvFj1BDjixZhFMP9huPvplqhakWfD3j5WrLVC1Jj4nZsXCIO2kaz5fvG3Ptxb74uQiJRsmO7FFuzNWVXcfcxdEl3buw4rXZ6jTj88jUk/Pnz898xn6zOvHa/v7+rGW+/fPPPz9W5tXBSVUwo7WRL0nXfy71qCfbegt5foLULt3d3Xb06FH7zGc+Y2VlZVZWVmY7d+60hx9+2MrKyqy+vt7OnDkTmyQzs76+PmtoaBi1zoqKCquqqor9CCGEEGLqEnTycd1119krr7wS+93tt99uS5YssXvvvdeampps5syZ1tnZaatXrzYzswMHDtjhw4etpaWlcL0WQgghxKQlaPMxb948W7p0aex3c+fOtbq6uszv77jjDtuwYYPV1tZaVVWV3XXXXdbS0hLk6SKEEEKIqUvBw6s/+OCDVlpaaqtXr7bBwUFbtWqVPfLII8H1lJSU5J2d7xz5ZmgslItuvvYg+YZCzzf0er51MsaSHdPbeaD76ptvvpn5jPK95pprspZ5OxqmZ0Z7G1yD3oUR2/C6bQyx7PWuqMtl/WE6VJZxFufNuw0yPT+zQUCYnt/DMuViObMHYn1j65TZR+RypWXjYm0ktdVB108/F1inrwfdQEOyKDN7nJCQ/dnuw7nw7ulmI8OWJ8XPBbNxYvaIaH9y3nnnZb0W3e5xHJ533nkn8xmffd+35ubmWBmzAWFyYs8lvjPZe8ITYkfkr8337/SYNx8vvPBC7PusWbOso6PDOjo6xlq1EEIIIaYgyu0ihBBCiFSZFFltxzuTbYiLUQiFyiSbjbQjuIZQKJfFQ4cOxb77KIO1tbWxMubCyPrDokqGRIr0R+FJ28Pv7Igc7/OuvixSJoLrxrvh4n0+WmOhjuiZbHK5YSclaZbTXO2HZGf2ePVJiKsvU9ewTLl4rW8f1QVJj+FD3OX92keVEAaO9GNmWVbZ+w1VC/45xQijXm7s2cO+4Zr242KqBvT29PdhnT4mFvbbq2/w2cPx+2uxDS9H9rcu378nvo6Qv506+RBCCCFEqmjzIYQQQohU0eZDCCGEEKkyKWw+PONh5zDeNiWFJN/xM11fiHtlUlmFuEyia5x3McOcQF5/jbps5oqXNOsp9o25haIbpnebxNDrrE5vc5E0ZLkZ18kzXTbW4+UfYrvBSJod1Syur0cbCG+7gGNKmoaAZUZmNgd4LwvFjnYOXqboeulh6wRJmp0Ur2VuuSzsPsuijDYH/plFF1F8Tn092H7SzLmIf/bQRdbPBbq2svGzaNs4Rr9usE5fz5EjR2JlXjbYbx8VHN+R7H2eNF1CLsYa6iIXOvkQQgghRKpo8yGEEEKIVJl0apdCMR6qlvF2rU0LdvSf71GcP4pEOeExYV9fX+bzsWPHYmX+CBmPPn0Zuq3l6/rLjvZDVBRefYDjZ0emvt/sPpwz5sLJ3CSZaiFX9FcPc3VlKiHmzouw7MdJ3YBxDYW4LDP8mDCqJovM6tvHfvsj81zPoZcjkz+qZLLVgX1Fufkx4tE+fk+anRbxqhV0ga+srBz1OryWqVkQ7wZrFleLoPrEyxHH4MeP7yXvhoz99nOMiVlx/L5NbJ9Fn/WMJSp1PujkQwghhBCpos2HEEIIIVJlwqldzh31eSvwQkRhy9ZOIUlD7VKofhciMRCDHftjYjX0BvCW9CxyI1rc+yNMbN8fE+N9/sg0l/rC9weP030ZtuH7jeP3beB9rM6Q54Jdy5LeMXWRXzeoIghRu/h6cH1jf7LVwzyBmNoFvU1YBEzmtcPWaUid46F2QRn6vrJ6mNoF5eafIVSzoGqHeRCxd6jvK8rNP4uoWvHjZ95UZvG5QdUSS8iXVAWK7fsxsYST7NnHe5naJSTycsjaOMe5Pif5O1USTTA/03feeceampqK3Q0hhBBC5EFPT48tXLiQXjPhNh/Dw8N25MgRi6LImpubraenh/pbT0cGBgasqalJshkFySY7kk12JJvRkVyyI9mMJIoiO378uDU2NuY0YJ1wapfS0lJbuHBh5gi9qqpKE5sFySY7kk12JJvsSDajI7lkR7KJU11dneg6GZwKIYQQIlW0+RBCCCFEqkzYzUdFRYX99V//9YhgKkKyYUg22ZFssiPZjI7kkh3JZmxMOINTIYQQQkxtJuzJhxBCCCGmJtp8CCGEECJVtPkQQgghRKpo8yGEEEKIVJmwm4+Ojg5btGiRzZo1y1auXGl79uwpdpdSpb293T772c/avHnzbMGCBXbzzTfbgQMHYtecPn3a2trarK6uziorK2316tWxdPTThU2bNllJSYmtX78+87vpLJt3333X/uiP/sjq6ups9uzZdtlll9m+ffsy5VEU2f33328XXHCBzZ4921pbW+3gwYNF7HE6DA0N2X333WeLFy+22bNn26//+q/b3/zN38TyUEwX2bz44ot24403WmNjo5WUlNiTTz4ZK08ih48++sjWrFljVVVVVlNTY3fccYedOHEixVGMD0w2Z8+etXvvvdcuu+wymzt3rjU2Ntptt91mR44cidUxVWVTUKIJyLZt26Ly8vLon//5n6PXXnst+tM//dOopqYm6uvrK3bXUmPVqlXRli1boldffTXav39/9Hu/93tRc3NzdOLEicw1d955Z9TU1BR1dnZG+/bti66++uroc5/7XBF7nT579uyJFi1aFF1++eXR3Xffnfn9dJXNRx99FF144YXRV7/61Wj37t3R22+/HT377LPRW2+9lblm06ZNUXV1dfTkk09GP//5z6MvfelL0eLFi6NTp04VsefjzwMPPBDV1dVFTz/9dHTo0KFo+/btUWVlZfS9730vc810kc1//ud/Rt/61reiH//4x5GZRU888USsPIkcvvjFL0ZXXHFFtGvXrui///u/o4suuii69dZbUx5J4WGy6e/vj1pbW6Mf/ehH0RtvvBF1dXVFK1asiJYtWxarY6rKppBMyM3HihUrora2tsz3oaGhqLGxMWpvby9ir4rL0aNHIzOLdu7cGUXR/z0EM2fOjLZv35655n//938jM4u6urqK1c1UOX78eHTxxRdHO3bsiH7rt34rs/mYzrK59957o89//vNZy4eHh6OGhobo7//+7zO/6+/vjyoqKqJ/+7d/S6OLReOGG26I/uRP/iT2u1tuuSVas2ZNFEXTVzb4BzaJHF5//fXIzKK9e/dmrvnpT38alZSURO+++25qfR9vRtuYIXv27InMLPrlL38ZRdH0kc1YmXBqlzNnzlh3d7e1trZmfldaWmqtra3W1dVVxJ4Vl2PHjpmZWW1trZmZdXd329mzZ2NyWrJkiTU3N08bObW1tdkNN9wQk4HZ9JbNf/zHf9jy5cvtD/7gD2zBggV21VVX2Q9+8INM+aFDh6y3tzcmm+rqalu5cuWUl83nPvc56+zstDfffNPMzH7+85/bSy+9ZNdff72ZTW/ZeJLIoaury2pqamz58uWZa1pbW620tNR2796dep+LybFjx6ykpMRqamrMTLJJyoRLLPfBBx/Y0NCQ1dfXx35fX19vb7zxRpF6VVyGh4dt/fr1ds0119jSpUvNzKy3t9fKy8szC/4c9fX11tvbW4Repsu2bdvsZz/7me3du3dE2XSWzdtvv22bN2+2DRs22F/+5V/a3r177c///M+tvLzc1q5dmxn/aM/XVJfNN7/5TRsYGLAlS5bYjBkzbGhoyB544AFbs2aNmdm0lo0niRx6e3ttwYIFsfKysjKrra2dVrI6ffq03XvvvXbrrbdmkstJNsmYcJsPMZK2tjZ79dVX7aWXXip2VyYEPT09dvfdd9uOHTts1qxZxe7OhGJ4eNiWL19uf/u3f2tmZldddZW9+uqr9uijj9ratWuL3Lvi8u///u/2wx/+0LZu3Wqf/vSnbf/+/bZ+/XprbGyc9rIR4Zw9e9b+8A//0KIoss2bNxe7O5OOCad2mT9/vs2YMWOEZ0JfX581NDQUqVfFY926dfb000/b888/bwsXLsz8vqGhwc6cOWP9/f2x66eDnLq7u+3o0aP2mc98xsrKyqysrMx27txpDz/8sJWVlVl9ff20lc0FF1xgl156aex3l1xyiR0+fNjMLDP+6fh8/cVf/IV985vftK985St22WWX2R//8R/bPffcY+3t7WY2vWXjSSKHhoYGO3r0aKz8k08+sY8++mhayOrcxuOXv/yl7dixI3PqYSbZJGXCbT7Ky8tt2bJl1tnZmfnd8PCwdXZ2WktLSxF7li5RFNm6devsiSeesOeee84WL14cK1+2bJnNnDkzJqcDBw7Y4cOHp7ycrrvuOnvllVds//79mZ/ly5fbmjVrMp+nq2yuueaaES7Zb775pl144YVmZrZ48WJraGiIyWZgYMB279495WXz8ccfW2lp/JU3Y8YMGx4eNrPpLRtPEjm0tLRYf3+/dXd3Z6557rnnbHh42FauXJl6n9Pk3Mbj4MGD9l//9V9WV1cXK5/Osgmi2Bavo7Ft27aooqIievzxx6PXX389+trXvhbV1NREvb29xe5aanz961+PqquroxdeeCF67733Mj8ff/xx5po777wzam5ujp577rlo3759UUtLS9TS0lLEXhcP7+0SRdNXNnv27InKysqiBx54IDp48GD0wx/+MJozZ070r//6r5lrNm3aFNXU1EQ/+clPov/5n/+JbrrppinpToqsXbs2+tSnPpVxtf3xj38czZ8/P/rGN76RuWa6yOb48ePRyy+/HL388suRmUX/8A//EL388ssZj40kcvjiF78YXXXVVdHu3bujl156Kbr44ounhDspk82ZM2eiL33pS9HChQuj/fv3x97Ng4ODmTqmqmwKyYTcfERRFH3/+9+Pmpubo/Ly8mjFihXRrl27it2lVDGzUX+2bNmSuebUqVPRn/3Zn0XnnXdeNGfOnOj3f//3o/fee694nS4iuPmYzrJ56qmnoqVLl0YVFRXRkiVLon/6p3+KlQ8PD0f33XdfVF9fH1VUVETXXXdddODAgSL1Nj0GBgaiu+++O2pubo5mzZoV/dqv/Vr0rW99K/ZHY7rI5vnnnx/1/bJ27dooipLJ4cMPP4xuvfXWqLKyMqqqqopuv/326Pjx40UYTWFhsjl06FDWd/Pzzz+fqWOqyqaQlESRC+8nhBBCCDHOTDibDyGEEEJMbbT5EEIIIUSqaPMhhBBCiFTR5kMIIYQQqaLNhxBCCCFSRZsPIYQQQqSKNh9CCCGESBVtPoQQQgiRKtp8CCGEECJVtPkQQgghRKpo8yGEEEKIVNHmQwghhBCp8v8AWfraxDylupgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignments (numeric): [1, 8, 13, 38, 1, 11, 20, 4, 38, 0, 19, 38, 11, 38, 18, 8, 23, 38, 13, 14, 22]\n",
      "Alignments (characters): bin blue at l six now\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test path for visualization\n",
    "test_path = '.\\\\data\\\\s1\\\\bbal6n.mpg'\n",
    "\n",
    "# Load frames and alignments using the load_data function\n",
    "frames, alignments = load_data(test_path)\n",
    "\n",
    "# Visualize a specific frame (e.g., the 40th frame)\n",
    "plt.imshow(frames[40], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Print alignments\n",
    "print(\"Alignments (numeric):\", alignments)\n",
    "\n",
    "# Convert alignments from numbers to characters\n",
    "alignment_chars = ''.join([num_to_char[num] for num in alignments])\n",
    "print(\"Alignments (characters):\", alignment_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a7eb4-0c3e-4eab-9291-5611cb68ce08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Create Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f066fea2-91b1-42ed-a67d-00566a1a53ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Custom Dataset class for PyTorch\n",
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        frames, alignments = load_data(path)\n",
    "        return torch.tensor(frames, dtype=torch.float32), torch.tensor(alignments, dtype=torch.long)\n",
    "\n",
    "# List all video files\n",
    "file_paths = glob.glob('.\\\\data\\\\s1\\\\*.mpg')\n",
    "random.shuffle(file_paths)\n",
    "\n",
    "# Split into training and testing\n",
    "train_paths = file_paths[:450]\n",
    "test_paths = file_paths[450:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LipReadingDataset(train_paths)\n",
    "test_dataset = LipReadingDataset(test_paths)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47733c-83bc-465c-b118-b198b492ad37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Design the Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e9a497-191b-4842-afbd-26f5e13c43ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LipReadingModel(\n",
      "  (conv3d_1): Conv3d(1, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool3d_1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3d_2): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (maxpool3d_2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3d_3): Conv3d(256, 75, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (maxpool3d_3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (lstm_1): LSTM(6375, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout_1): Dropout(p=0.5, inplace=False)\n",
      "  (lstm_2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout_2): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=40, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LipReadingModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LipReadingModel, self).__init__()\n",
    "        # 3D Convolutional layers\n",
    "        self.conv3d_1 = nn.Conv3d(1, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool3d_1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "\n",
    "        self.conv3d_2 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.maxpool3d_2 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "\n",
    "        self.conv3d_3 = nn.Conv3d(256, 75, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.maxpool3d_3 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "\n",
    "        # Bi-directional LSTM layers\n",
    "        self.lstm_1 = nn.LSTM(6375, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout_1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.lstm_2 = nn.LSTM(256, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(256, vocab_size + 1)  # +1 for CTC blank token\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, depth, height, width)\n",
    "        x = self.relu(self.conv3d_1(x))\n",
    "        x = self.maxpool3d_1(x)\n",
    "\n",
    "        x = self.relu(self.conv3d_2(x))\n",
    "        x = self.maxpool3d_2(x)\n",
    "\n",
    "        x = self.relu(self.conv3d_3(x))\n",
    "        x = self.maxpool3d_3(x)\n",
    "\n",
    "        # Flatten the features for LSTM input\n",
    "        batch_size, channels, depth, height, width = x.size()\n",
    "        x = x.view(batch_size, depth, channels * height * width)  # (batch_size, depth, features)\n",
    "\n",
    "        # BiLSTM layers\n",
    "        x, _ = self.lstm_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "\n",
    "        x, _ = self.lstm_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "model = LipReadingModel(vocab_size)\n",
    "model = model.to(device)  # Move the model to GPU if available\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c2eae0-c359-41a4-97a0-75c44dccb7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the CTC Loss function as in the original notebook\n",
    "# 'blank=0' for the CTC blank token, 'reduction' is set to 'mean' as typically done for loss averaging.\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "\n",
    "# Define the optimizer\n",
    "# Assuming 'model' is already defined elsewhere in your code\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Optional: Learning Rate Scheduler\n",
    "# Exponential decay of the learning rate\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)  # gamma is the decay factor\n",
    "\n",
    "# Usage in your training loop\n",
    "def compute_loss(ctc_output, ctc_targets, input_lengths, target_lengths):\n",
    "    # Compute the CTC loss, where ctc_output is of shape (T, N, C)\n",
    "    # ctc_targets is a flattened tensor of target labels for the entire batch\n",
    "    # input_lengths and target_lengths are 1D tensors indicating the lengths of each input and target respectively\n",
    "    ctc_loss = ctc_loss_fn(ctc_output, ctc_targets, input_lengths, target_lengths)\n",
    "    return ctc_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec02176-5c26-46c3-aff7-8352e6563c7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Setup Training Options and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffba483-aa61-4bbe-a15f-a73e1ddf097c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/450 [00:00<?, ?batch/s]C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_3312\\2377108712.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(frames, dtype=torch.float32), torch.tensor(alignments, dtype=torch.long)\n",
      "Epoch 1/100:  33%|███▎      | 148/450 [01:01<01:52,  2.69batch/s, loss=1.77] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer  # Import jiwer for WER calculation\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # Adjust as needed\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "with open(\"validation_results.txt\", \"w\") as log_file:\n",
    "    for epoch in range(num_epochs):\n",
    "        try:\n",
    "            epoch_loss = 0  # Track the total loss for the epoch\n",
    "\n",
    "            with tqdm(train_loader, unit=\"batch\") as progress_bar:\n",
    "                progress_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "                for batch in progress_bar:\n",
    "                    frames, alignments = batch\n",
    "                    frames = frames[0].to(device)  # Move frames to the device (single video sequence)\n",
    "                    alignments = alignments[0].to(device)  # Move alignments to the device (single sequence)\n",
    "\n",
    "                    frames = frames.unsqueeze(0).unsqueeze(1)  # (batch_size=1, channels=1, depth, height, width)\n",
    "                    input_length = torch.tensor([frames.size(2)], dtype=torch.long).to(device)\n",
    "                    target_length = torch.tensor([len(alignments)], dtype=torch.long).to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(frames)  # Outputs shape: (batch_size, seq_length, num_classes)\n",
    "                    outputs = outputs.permute(1, 0, 2)  # (seq_length, batch_size, num_classes)\n",
    "\n",
    "                    # Compute the CTC loss using the provided compute_loss function\n",
    "                    loss = compute_loss(outputs, alignments, input_length, target_length)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            # Learning rate scheduler step (if using)\n",
    "            scheduler.step()\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss}\")\n",
    "\n",
    "            # Save the model weights after every epoch (overwrite the same file)\n",
    "            torch.save(model.state_dict(), \"lip_reading_model_weights.pth\")\n",
    "            print(f\"Model weights updated and saved.\")\n",
    "\n",
    "            # Validate the model every 5 epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                all_predictions = []\n",
    "                all_references = []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in test_loader:\n",
    "                        val_frames, val_alignments = val_batch\n",
    "                        val_frames = val_frames[0].to(device)\n",
    "                        val_alignments = val_alignments[0].to(device)\n",
    "\n",
    "                        val_frames = val_frames.unsqueeze(0).unsqueeze(1)\n",
    "                        val_input_length = torch.tensor([val_frames.size(2)], dtype=torch.long).to(device)\n",
    "                        val_target_length = torch.tensor([len(val_alignments)], dtype=torch.long).to(device)\n",
    "\n",
    "                        val_outputs = model(val_frames)\n",
    "                        val_outputs = val_outputs.permute(1, 0, 2)\n",
    "                        val_loss += compute_loss(val_outputs, val_alignments, val_input_length, val_target_length).item()\n",
    "\n",
    "                        predicted_indices = torch.argmax(val_outputs, dim=-1).squeeze().tolist()\n",
    "                        predicted_text = ''.join([num_to_char[i] for i in predicted_indices if i != 0])\n",
    "                        true_text = ''.join([num_to_char[i] for i in val_alignments.cpu().numpy()])\n",
    "\n",
    "                        all_predictions.append(predicted_text)\n",
    "                        all_references.append(true_text)\n",
    "\n",
    "                        log_file.write(f\"Epoch {epoch + 1}\\n\")\n",
    "                        log_file.write(f\"Prediction: {predicted_text}\\n\")\n",
    "                        log_file.write(f\"Label: {true_text}\\n\")\n",
    "                        log_file.write(\"\\n\")\n",
    "\n",
    "                avg_val_loss = val_loss / len(test_loader)\n",
    "                print(f\"Validation Loss at Epoch {epoch + 1}: {avg_val_loss}\")\n",
    "\n",
    "                wer_value = wer(all_references, all_predictions)\n",
    "                print(f\"Word Error Rate (WER) at Epoch {epoch + 1}: {wer_value}\")\n",
    "\n",
    "                print(\"\\nSample Predictions and Labels:\")\n",
    "                for i in range(min(5, len(all_predictions))):\n",
    "                    print(f\"Prediction: {all_predictions[i]}\")\n",
    "                    print(f\"Label: {all_references[i]}\\n\")\n",
    "\n",
    "                model.train()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoch {epoch + 1}: {e}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bda642",
   "metadata": {},
   "source": [
    "# Prediction and Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9894874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_19144\\1026220533.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f, map_location=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LipReadingModel:\n\tMissing key(s) in state_dict: \"conv3d_1.weight\", \"conv3d_1.bias\", \"conv3d_2.weight\", \"conv3d_2.bias\", \"conv3d_3.weight\", \"conv3d_3.bias\", \"lstm_1.weight_ih_l0\", \"lstm_1.weight_hh_l0\", \"lstm_1.bias_ih_l0\", \"lstm_1.bias_hh_l0\", \"lstm_1.weight_ih_l0_reverse\", \"lstm_1.weight_hh_l0_reverse\", \"lstm_1.bias_ih_l0_reverse\", \"lstm_1.bias_hh_l0_reverse\", \"lstm_2.weight_ih_l0\", \"lstm_2.weight_hh_l0\", \"lstm_2.bias_ih_l0\", \"lstm_2.bias_hh_l0\", \"lstm_2.weight_ih_l0_reverse\", \"lstm_2.weight_hh_l0_reverse\", \"lstm_2.bias_ih_l0_reverse\", \"lstm_2.bias_hh_l0_reverse\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\", \"gru1.weight_ih_l0\", \"gru1.weight_hh_l0\", \"gru1.bias_ih_l0\", \"gru1.bias_hh_l0\", \"gru1.weight_ih_l0_reverse\", \"gru1.weight_hh_l0_reverse\", \"gru1.bias_ih_l0_reverse\", \"gru1.bias_hh_l0_reverse\", \"gru2.weight_ih_l0\", \"gru2.weight_hh_l0\", \"gru2.bias_ih_l0\", \"gru2.bias_hh_l0\", \"gru2.weight_ih_l0_reverse\", \"gru2.weight_hh_l0_reverse\", \"gru2.bias_ih_l0_reverse\", \"gru2.bias_hh_l0_reverse\", \"FC.weight\", \"FC.bias\", \"coord_gru.weight_ih_l0\", \"coord_gru.weight_hh_l0\", \"coord_gru.bias_ih_l0\", \"coord_gru.bias_hh_l0\", \"coord_gru.weight_ih_l0_reverse\", \"coord_gru.weight_hh_l0_reverse\", \"coord_gru.bias_ih_l0_reverse\", \"coord_gru.bias_hh_l0_reverse\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Load the pre-trained weights from the pickle file\u001b[39;00m\n\u001b[0;32m     80\u001b[0m weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPython Code\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLipCoordNet_coords_loss_0.025581153109669685_wer_0.01746208431890914_cer_0.006488426950253695.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mload_model_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Path to the video and .align file\u001b[39;00m\n\u001b[0;32m     84\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPython Code\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msrbb4n.mpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mload_model_weights\u001b[1;34m(model, weight_path, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(weight_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\Python Code\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LipReadingModel:\n\tMissing key(s) in state_dict: \"conv3d_1.weight\", \"conv3d_1.bias\", \"conv3d_2.weight\", \"conv3d_2.bias\", \"conv3d_3.weight\", \"conv3d_3.bias\", \"lstm_1.weight_ih_l0\", \"lstm_1.weight_hh_l0\", \"lstm_1.bias_ih_l0\", \"lstm_1.bias_hh_l0\", \"lstm_1.weight_ih_l0_reverse\", \"lstm_1.weight_hh_l0_reverse\", \"lstm_1.bias_ih_l0_reverse\", \"lstm_1.bias_hh_l0_reverse\", \"lstm_2.weight_ih_l0\", \"lstm_2.weight_hh_l0\", \"lstm_2.bias_ih_l0\", \"lstm_2.bias_hh_l0\", \"lstm_2.weight_ih_l0_reverse\", \"lstm_2.weight_hh_l0_reverse\", \"lstm_2.bias_ih_l0_reverse\", \"lstm_2.bias_hh_l0_reverse\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\", \"gru1.weight_ih_l0\", \"gru1.weight_hh_l0\", \"gru1.bias_ih_l0\", \"gru1.bias_hh_l0\", \"gru1.weight_ih_l0_reverse\", \"gru1.weight_hh_l0_reverse\", \"gru1.bias_ih_l0_reverse\", \"gru1.bias_hh_l0_reverse\", \"gru2.weight_ih_l0\", \"gru2.weight_hh_l0\", \"gru2.bias_ih_l0\", \"gru2.bias_hh_l0\", \"gru2.weight_ih_l0_reverse\", \"gru2.weight_hh_l0_reverse\", \"gru2.bias_ih_l0_reverse\", \"gru2.bias_hh_l0_reverse\", \"FC.weight\", \"FC.bias\", \"coord_gru.weight_ih_l0\", \"coord_gru.weight_hh_l0\", \"coord_gru.bias_ih_l0\", \"coord_gru.bias_hh_l0\", \"coord_gru.weight_ih_l0_reverse\", \"coord_gru.weight_hh_l0_reverse\", \"coord_gru.bias_ih_l0_reverse\", \"coord_gru.bias_hh_l0_reverse\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the trained model\n",
    "def load_model_weights(model, weight_path, device):\n",
    "    with open(weight_path, 'rb') as f:\n",
    "        state_dict = torch.load(f, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to preprocess the video\n",
    "def preprocess_video(video_path, device):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        frame = frame[190:236, 80:220]  # Crop to focus on the mouth region\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Convert frames to a Torch tensor and normalize\n",
    "    frames = torch.tensor(frames, dtype=torch.float32)  # Shape: (depth, height, width)\n",
    "    frames = frames.unsqueeze(1)  # Add channel dimension: (depth, 1, height, width)\n",
    "    \n",
    "    # Normalize the frames\n",
    "    mean = frames.mean()\n",
    "    std = frames.std()\n",
    "    frames = (frames - mean) / std  # Standardize frames\n",
    "\n",
    "    # Rearrange dimensions to (batch_size, channels, depth, height, width)\n",
    "    frames = frames.permute(1, 0, 2, 3).unsqueeze(0).to(device)  # (1, 1, depth, height, width)\n",
    "    return frames\n",
    "\n",
    "# Function to extract lip coordinates from a .align file\n",
    "def extract_lip_coordinates(align_path):\n",
    "    coordinates = []\n",
    "    with open(align_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            if len(values) == 1:  # If the line has a single value, it may represent a coordinate\n",
    "                coordinates.append(float(values[0]))\n",
    "    return torch.tensor(coordinates, dtype=torch.float32)\n",
    "\n",
    "# Function to make predictions using the model\n",
    "def predict(video_path, align_path, model, device):\n",
    "    # Preprocess the input video\n",
    "    frames = preprocess_video(video_path, device)\n",
    "    # Load lip coordinates\n",
    "    lip_coordinates = extract_lip_coordinates(align_path).to(device)\n",
    "\n",
    "    # Combine frames and lip coordinates if needed\n",
    "    # Note: Modify this section based on how your model uses lip coordinates\n",
    "    inputs = (frames, lip_coordinates.unsqueeze(0))  # Add batch dimension to coordinates\n",
    "\n",
    "    # Make predictions with no gradient computation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(*inputs)  # Forward pass through the model\n",
    "        outputs = outputs.permute(1, 0, 2)  # Reshape for CTC decoding: (seq_length, batch_size, num_classes)\n",
    "\n",
    "    # Get the predicted character indices\n",
    "    predicted_indices = torch.argmax(outputs, dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Decode the indices into characters\n",
    "    predicted_text = ''.join([num_to_char[i] for i in predicted_indices if i != 0])\n",
    "    return predicted_text\n",
    "\n",
    "# Load the model and set it to the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(vocab)  # Ensure vocab is defined\n",
    "model = LipReadingModel(vocab_size=vocab_size)  # Replace with your model class\n",
    "\n",
    "# Load the pre-trained weights from the pickle file\n",
    "weight_path = \"D:\\Python Code\\LipCoordNet_coords_loss_0.025581153109669685_wer_0.01746208431890914_cer_0.006488426950253695.pt\"\n",
    "load_model_weights(model, weight_path, device)\n",
    "\n",
    "# Path to the video and .align file\n",
    "video_path = \"D:\\Python Code\\data\\s1\\srbb4n.mpg\"\n",
    "align_path = \"D:\\Python Code\\data\\alignments\\s1\\srbb4n.mpg\"\n",
    "\n",
    "# Make predictions\n",
    "predicted_text = predict(video_path, align_path, model, device)\n",
    "print(\"Predicted Text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a40635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
